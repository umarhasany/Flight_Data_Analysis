{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff28f90-6163-4e52-96e5-a0d774884bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.3\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.makedirs(\"/tmp/spark-local\", exist_ok=True)\n",
    "os.makedirs(\"/tmp/spark-warehouse\", exist_ok=True)\n",
    "os.makedirs(\"/tmp/spark-checkpoints\", exist_ok=True)\n",
    "\n",
    "GRAPHFRAMES_PACKAGE = \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\"\n",
    "DELTA_PACKAGE      = \"io.delta:delta-spark_2.12:3.0.0\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"DeltaPlusGraphFrames\")\n",
    "    .master(\"local[*]\")\n",
    "\n",
    "    # Packages (both)\n",
    "    .config(\"spark.jars.packages\", f\"{DELTA_PACKAGE},{GRAPHFRAMES_PACKAGE}\")\n",
    "\n",
    "    # Delta Lake requirements\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "    # Local stability/perf basics\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\")\n",
    "    .config(\"spark.default.parallelism\", \"32\")\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-local\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "spark.range(3).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbf6bb5-dd62-49f7-a720-90eb1f76a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphFrames imported OK\n"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "print(\"GraphFrames imported OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b0eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['FL_DATE', 'OP_CARRIER', 'OP_CARRIER_FL_NUM', 'ORIGIN', 'DEST', 'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'Unnamed: 27']\n",
      "Number of rows: 61556964\n",
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: double (nullable = true)\n",
      " |-- DEP_TIME: double (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- CRS_ARR_TIME: double (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- CARRIER_DELAY: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- SECURITY_DELAY: double (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: double (nullable = true)\n",
      " |-- Unnamed: 27: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>...</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>Unnamed: 27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>XE</td>\n",
       "      <td>1204</td>\n",
       "      <td>DCA</td>\n",
       "      <td>EWR</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>XE</td>\n",
       "      <td>1206</td>\n",
       "      <td>EWR</td>\n",
       "      <td>IAD</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1509.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1537.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>XE</td>\n",
       "      <td>1207</td>\n",
       "      <td>EWR</td>\n",
       "      <td>DCA</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1059.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>XE</td>\n",
       "      <td>1208</td>\n",
       "      <td>DCA</td>\n",
       "      <td>EWR</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1249.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1259.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>XE</td>\n",
       "      <td>1209</td>\n",
       "      <td>IAD</td>\n",
       "      <td>EWR</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1729.0</td>\n",
       "      <td>...</td>\n",
       "      <td>105.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FL_DATE OP_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  CRS_DEP_TIME  \\\n",
       "0  2009-01-01         XE               1204    DCA  EWR        1100.0   \n",
       "1  2009-01-01         XE               1206    EWR  IAD        1510.0   \n",
       "2  2009-01-01         XE               1207    EWR  DCA        1100.0   \n",
       "3  2009-01-01         XE               1208    DCA  EWR        1240.0   \n",
       "4  2009-01-01         XE               1209    IAD  EWR        1715.0   \n",
       "\n",
       "   DEP_TIME  DEP_DELAY  TAXI_OUT  WHEELS_OFF  ...  CRS_ELAPSED_TIME  \\\n",
       "0    1058.0       -2.0      18.0      1116.0  ...              62.0   \n",
       "1    1509.0       -1.0      28.0      1537.0  ...              82.0   \n",
       "2    1059.0       -1.0      20.0      1119.0  ...              70.0   \n",
       "3    1249.0        9.0      10.0      1259.0  ...              77.0   \n",
       "4    1705.0      -10.0      24.0      1729.0  ...             105.0   \n",
       "\n",
       "   ACTUAL_ELAPSED_TIME  AIR_TIME  DISTANCE  CARRIER_DELAY  WEATHER_DELAY  \\\n",
       "0                 68.0      42.0     199.0            NaN            NaN   \n",
       "1                 75.0      43.0     213.0            NaN            NaN   \n",
       "2                 62.0      36.0     199.0            NaN            NaN   \n",
       "3                 56.0      37.0     199.0            NaN            NaN   \n",
       "4                 77.0      40.0     213.0            NaN            NaN   \n",
       "\n",
       "  NAS_DELAY  SECURITY_DELAY  LATE_AIRCRAFT_DELAY  Unnamed: 27  \n",
       "0       NaN             NaN                  NaN         None  \n",
       "1       NaN             NaN                  NaN         None  \n",
       "2       NaN             NaN                  NaN         None  \n",
       "3       NaN             NaN                  NaN         None  \n",
       "4       NaN             NaN                  NaN         None  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV into a DataFrame\n",
    "# Replace path with the actual path inside your container if different.\n",
    "csv_path = \"/home/jovyan/*.csv\"  # read all CSV files in the folder\n",
    " # <-- change if needed\n",
    "# Example columns expected (adjust according to the Kaggle dataset you downloaded):\n",
    "# FlightDate, Airline, FlightNum, Origin, Dest, Cancelled, Diverted, etc.\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
    "print(\"Columns:\", df.columns)\n",
    "print(\"Number of rows:\", df.count())\n",
    "df.printSchema()\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1d15be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct edges: 7956\n",
      "Number of vertices (airports): 381\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JFK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MBS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EVV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SAF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>COU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id\n",
       "0  MEM\n",
       "1  JFK\n",
       "2  MBS\n",
       "3  JAX\n",
       "4  ANC\n",
       "5  HPN\n",
       "6  EVV\n",
       "7  SBN\n",
       "8  SAF\n",
       "9  COU"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Preprocess to create edges and vertices\n",
    "# We'll aggregate edges by (origin, dest) and count the number of flights (weight).\n",
    "edges = df.select(F.col(\"Origin\").alias(\"src\"), F.col(\"Dest\").alias(\"dst\")) \\\n",
    "          .filter(F.col(\"src\").isNotNull() & F.col(\"dst\").isNotNull())\n",
    "\n",
    "# Aggregate to get weights\n",
    "edge_counts = edges.groupBy(\"src\", \"dst\").count().withColumnRenamed(\"count\", \"weight\")\n",
    "print(\"Number of distinct edges:\", edge_counts.count())\n",
    "edge_counts.limit(5).toPandas()\n",
    "\n",
    "# Create vertices DataFrame (unique airports)\n",
    "src_verts = edge_counts.select(F.col(\"src\").alias(\"id\"))\n",
    "dst_verts = edge_counts.select(F.col(\"dst\").alias(\"id\"))\n",
    "vertices = src_verts.union(dst_verts).distinct()\n",
    "print(\"Number of vertices (airports):\", vertices.count())\n",
    "vertices.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8626ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>out_degree</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>total_degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL</td>\n",
       "      <td>3903244</td>\n",
       "      <td>3903288</td>\n",
       "      <td>7806532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORD</td>\n",
       "      <td>3001285</td>\n",
       "      <td>3001372</td>\n",
       "      <td>6002657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DFW</td>\n",
       "      <td>2546075</td>\n",
       "      <td>2546050</td>\n",
       "      <td>5092125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEN</td>\n",
       "      <td>2300550</td>\n",
       "      <td>2300456</td>\n",
       "      <td>4601006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LAX</td>\n",
       "      <td>2133445</td>\n",
       "      <td>2133646</td>\n",
       "      <td>4267091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PHX</td>\n",
       "      <td>1720614</td>\n",
       "      <td>1720588</td>\n",
       "      <td>3441202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IAH</td>\n",
       "      <td>1672053</td>\n",
       "      <td>1672279</td>\n",
       "      <td>3344332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SFO</td>\n",
       "      <td>1612933</td>\n",
       "      <td>1613144</td>\n",
       "      <td>3226077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LAS</td>\n",
       "      <td>1472436</td>\n",
       "      <td>1472477</td>\n",
       "      <td>2944913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CLT</td>\n",
       "      <td>1334522</td>\n",
       "      <td>1334543</td>\n",
       "      <td>2669065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  out_degree  in_degree  total_degree\n",
       "0  ATL     3903244    3903288       7806532\n",
       "1  ORD     3001285    3001372       6002657\n",
       "2  DFW     2546075    2546050       5092125\n",
       "3  DEN     2300550    2300456       4601006\n",
       "4  LAX     2133445    2133646       4267091\n",
       "5  PHX     1720614    1720588       3441202\n",
       "6  IAH     1672053    1672279       3344332\n",
       "7  SFO     1612933    1613144       3226077\n",
       "8  LAS     1472436    1472477       2944913\n",
       "9  CLT     1334522    1334543       2669065"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute in-degree, out-degree and total degree using Spark aggregations (native implementation)\n",
    "outdeg = edge_counts.groupBy(\"src\").agg(F.sum(\"weight\").alias(\"out_degree\"))\n",
    "indeg = edge_counts.groupBy(\"dst\").agg(F.sum(\"weight\").alias(\"in_degree\"))\n",
    "\n",
    "# Rename columns for join\n",
    "outdeg = outdeg.withColumnRenamed(\"src\", \"id\")\n",
    "indeg = indeg.withColumnRenamed(\"dst\", \"id\")\n",
    "\n",
    "degrees = vertices.join(outdeg, \"id\", \"left\").join(indeg, \"id\", \"left\") \\\n",
    "                  .na.fill(0, subset=[\"out_degree\", \"in_degree\"]) \\\n",
    "                  .withColumn(\"total_degree\", F.col(\"in_degree\") + F.col(\"out_degree\"))\n",
    "\n",
    "degrees.orderBy(F.desc(\"total_degree\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca44fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triangles (undirected, counted once): 36562\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Total triangle count in the graph (native Spark DataFrame)\n",
    "# Treats the graph as undirected and counts each triangle exactly once.\n",
    "#\n",
    "# Input required:\n",
    "#   edge_counts: DataFrame with columns (src, dst, weight) or at least (src, dst)\n",
    "# Output:\n",
    "#   total_triangles: integer\n",
    "# =========================\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# 1) Canonical undirected edges: store each edge once as (u, v) with u < v\n",
    "undirected = (\n",
    "    edge_counts\n",
    "    .select(\n",
    "        F.when(F.col(\"src\") < F.col(\"dst\"), F.col(\"src\")).otherwise(F.col(\"dst\")).alias(\"u\"),\n",
    "        F.when(F.col(\"src\") < F.col(\"dst\"), F.col(\"dst\")).otherwise(F.col(\"src\")).alias(\"v\")\n",
    "    )\n",
    "    .filter(F.col(\"u\").isNotNull() & F.col(\"v\").isNotNull() & (F.col(\"u\") != F.col(\"v\")))\n",
    "    .distinct()\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "_ = undirected.count()  # materialize cache\n",
    "\n",
    "# 2) For each node, list neighbors as rows (both directions)\n",
    "neighbors = (\n",
    "    undirected.select(F.col(\"u\").alias(\"node\"), F.col(\"v\").alias(\"nbr\"))\n",
    "    .union(undirected.select(F.col(\"v\").alias(\"node\"), F.col(\"u\").alias(\"nbr\")))\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "_ = neighbors.count()\n",
    "\n",
    "# 3) Build wedges (u, v, w): edges (u, v) and (u, w) exist\n",
    "#    Use u from undirected edge and w from neighbors of u\n",
    "triples = (\n",
    "    undirected.alias(\"e\")\n",
    "    .join(neighbors.alias(\"n\"), F.col(\"e.u\") == F.col(\"n.node\"), \"inner\")\n",
    "    .select(\n",
    "        F.col(\"e.u\").alias(\"u\"),\n",
    "        F.col(\"e.v\").alias(\"v\"),\n",
    "        F.col(\"n.nbr\").alias(\"w\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4) Enforce ordering u < v < w so every triangle is generated once\n",
    "triples_filtered = triples.filter(\n",
    "    (F.col(\"v\") != F.col(\"w\")) &\n",
    "    (F.col(\"u\") < F.col(\"v\")) &\n",
    "    (F.col(\"v\") < F.col(\"w\"))\n",
    ")\n",
    "\n",
    "# 5) Close the wedge by checking that edge (v, w) exists in undirected\n",
    "triangles = (\n",
    "    triples_filtered.alias(\"t\")\n",
    "    .join(\n",
    "        undirected.alias(\"e2\"),\n",
    "        (F.col(\"t.v\") == F.col(\"e2.u\")) & (F.col(\"t.w\") == F.col(\"e2.v\")),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(F.col(\"t.u\").alias(\"u\"), F.col(\"t.v\").alias(\"v\"), F.col(\"t.w\").alias(\"w\"))\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# 6) Total number of triangles (counted once)\n",
    "total_triangles = triangles.count()\n",
    "print(\"Total triangles (undirected, counted once):\", total_triangles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed2173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|node|eigen_c            |\n",
      "+----+-------------------+\n",
      "|ATL |0.1734399146743071 |\n",
      "|ORD |0.17178708117153438|\n",
      "|DEN |0.16587975673931238|\n",
      "|DFW |0.15891414031202006|\n",
      "|MSP |0.15420328962929342|\n",
      "|DTW |0.1530555615432875 |\n",
      "|CLT |0.1520300715864211 |\n",
      "|IAH |0.14715076171603145|\n",
      "|LAS |0.1453291471581526 |\n",
      "|EWR |0.1438885501986357 |\n",
      "+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Centrality (non-PageRank) natively on Spark: Eigenvector Centrality (Power Iteration)\n",
    "#\n",
    "# Idea:\n",
    "# - Centrality score of a node is proportional to the sum of centrality scores of its neighbors.\n",
    "# - Power iteration update:\n",
    "#       c_new(dst) = Σ c_old(src) for all edges (src -> dst)\n",
    "# - After each iteration, L2-normalize to keep values bounded.\n",
    "#\n",
    "# Requires:\n",
    "# - edge_counts DataFrame with columns (src, dst) (weight optional; ignored here)\n",
    "# - vertices DataFrame with column (id) OR we build it from edge_counts\n",
    "#\n",
    "# Output:\n",
    "# - top 10 nodes by eigenvector centrality\n",
    "# Produces:\n",
    "# - evec DataFrame: (node, eigen_c)\n",
    "# =========================\n",
    "\n",
    "import math\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"32\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# Build vertices if missing\n",
    "if \"vertices\" not in globals():\n",
    "    vertices = (\n",
    "        edge_counts.select(F.col(\"src\").alias(\"id\"))\n",
    "        .union(edge_counts.select(F.col(\"dst\").alias(\"id\")))\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "# Treat graph as undirected for eigenvector centrality (common choice)\n",
    "edges = (\n",
    "    edge_counts.select(F.col(\"src\").alias(\"src\"), F.col(\"dst\").alias(\"dst\"))\n",
    "    .union(edge_counts.select(F.col(\"dst\").alias(\"src\"), F.col(\"src\").alias(\"dst\")))\n",
    "    .filter(F.col(\"src\").isNotNull() & F.col(\"dst\").isNotNull() & (F.col(\"src\") != F.col(\"dst\")))\n",
    "    .distinct()\n",
    "    .repartition(\"dst\")\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "\n",
    "verts = (\n",
    "    vertices.select(F.col(\"id\").alias(\"node\"))\n",
    "            .filter(F.col(\"node\").isNotNull())\n",
    "            .distinct()\n",
    "            .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "\n",
    "_ = edges.count()\n",
    "_ = verts.count()\n",
    "\n",
    "# Initialize centrality vector: c(node) = 1\n",
    "r = verts.withColumn(\"c\", F.lit(1.0)).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "_ = r.count()\n",
    "\n",
    "num_iters = 15\n",
    "\n",
    "for i in range(num_iters):\n",
    "\n",
    "    # Core eigenvector centrality update: c_new(dst) = sum of c_old(src) for incoming edges\n",
    "    contribs = (\n",
    "        edges.join(r, edges.src == r.node, \"inner\")\n",
    "             .groupBy(\"dst\")\n",
    "             .agg(F.sum(F.col(\"c\")).alias(\"c\"))\n",
    "    )\n",
    "\n",
    "    r_new = (\n",
    "        verts.join(contribs, verts.node == contribs.dst, \"left\")\n",
    "             .select(verts.node.alias(\"node\"), F.coalesce(F.col(\"c\"), F.lit(0.0)).alias(\"c\"))\n",
    "    )\n",
    "\n",
    "    # L2 normalization (one small action per iteration)\n",
    "    norm_sq = r_new.select(F.sum(F.col(\"c\") * F.col(\"c\")).alias(\"ns\")).first()[\"ns\"]\n",
    "    norm = math.sqrt(norm_sq) if norm_sq and norm_sq > 0 else 1.0\n",
    "\n",
    "    r_new = r_new.withColumn(\"c\", F.col(\"c\") / F.lit(norm)) \\\n",
    "                 .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    r.unpersist()\n",
    "    r = r_new\n",
    "\n",
    "# Result\n",
    "evec = r.select(F.col(\"node\"), F.col(\"c\").alias(\"eigen_c\"))\n",
    "evec.orderBy(F.desc(\"eigen_c\")).limit(10).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7992e8df-506a-447a-8a75-ac4862601449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|node|rank                |\n",
      "+----+--------------------+\n",
      "|ATL |0.058125417143436525|\n",
      "|ORD |0.046779129117865303|\n",
      "|DFW |0.04039777575019987 |\n",
      "|DEN |0.0362993068819135  |\n",
      "|LAX |0.028476336519615262|\n",
      "|IAH |0.02373947040723849 |\n",
      "|SFO |0.022906290038492084|\n",
      "|PHX |0.022886791507266187|\n",
      "|MSP |0.02096816056904428 |\n",
      "|DTW |0.02054116037671708 |\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# PageRank (native Spark DataFrame, weighted, dangling handled)\n",
    "# NO GraphFrames functions, NO checkpoint/localCheckpoint\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "os.makedirs(\"/tmp/spark-local\", exist_ok=True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")      # lower = less shuffle memory\n",
    "spark.conf.set(\"spark.default.parallelism\", \"16\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# Ensure weight exists\n",
    "if \"weight\" not in edge_counts.columns:\n",
    "    edge_counts = edge_counts.withColumn(\"weight\", F.lit(1.0))\n",
    "\n",
    "# Ensure vertices exists\n",
    "if \"vertices\" not in globals():\n",
    "    vertices = (\n",
    "        edge_counts.select(F.col(\"src\").alias(\"id\"))\n",
    "        .union(edge_counts.select(F.col(\"dst\").alias(\"id\")))\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "verts = (\n",
    "    vertices.select(F.col(\"id\").alias(\"node\"))\n",
    "            .filter(F.col(\"node\").isNotNull())\n",
    "            .distinct()\n",
    "            .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "\n",
    "edges = (\n",
    "    edge_counts.select(\"src\", \"dst\", \"weight\")\n",
    "               .filter(F.col(\"src\").isNotNull() & F.col(\"dst\").isNotNull() & (F.col(\"src\") != F.col(\"dst\")))\n",
    "               .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "\n",
    "outdeg = (\n",
    "    edges.groupBy(\"src\")\n",
    "         .agg(F.sum(\"weight\").alias(\"out_w\"))\n",
    "         .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "\n",
    "# Materialize once\n",
    "N = float(verts.count())\n",
    "_ = edges.count()\n",
    "_ = outdeg.count()\n",
    "\n",
    "# Dangling nodes: no outgoing edges\n",
    "dangling_nodes = (\n",
    "    verts.join(outdeg.select(F.col(\"src\").alias(\"node\")), on=\"node\", how=\"left_anti\")\n",
    "         .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "_ = dangling_nodes.count()\n",
    "\n",
    "# Pre-join edges with outdeg once (reduces per-iteration work)\n",
    "edges_norm = (\n",
    "    edges.join(outdeg, on=\"src\", how=\"inner\")\n",
    "         .select(\"src\", \"dst\", \"weight\", \"out_w\")\n",
    "         .repartition(\"dst\")\n",
    "         .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "_ = edges_norm.count()\n",
    "\n",
    "damping = 0.85\n",
    "base = (1.0 - damping) / N\n",
    "\n",
    "ranks = verts.withColumn(\"rank\", F.lit(1.0 / N)).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "_ = ranks.count()\n",
    "\n",
    "num_iters = 20\n",
    "\n",
    "for i in range(num_iters):\n",
    "    # Compute dangling mass as a Python float (tiny collect; avoids crossJoin and checkpoint)\n",
    "    dm = (\n",
    "        ranks.join(dangling_nodes, on=\"node\", how=\"inner\")\n",
    "             .agg(F.sum(\"rank\").alias(\"dm\"))\n",
    "             .first()[\"dm\"]\n",
    "    )\n",
    "    dm = float(dm) if dm is not None else 0.0\n",
    "    dangling_term = damping * dm / N\n",
    "\n",
    "    contribs = (\n",
    "        edges_norm.alias(\"e\")\n",
    "        .join(ranks.alias(\"r\"), F.col(\"e.src\") == F.col(\"r.node\"), \"inner\")\n",
    "        .select(\n",
    "            F.col(\"e.dst\").alias(\"node\"),\n",
    "            (F.col(\"r.rank\") * (F.col(\"e.weight\") / F.col(\"e.out_w\"))).alias(\"contrib\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    summed = contribs.groupBy(\"node\").agg(F.sum(\"contrib\").alias(\"sum_contrib\"))\n",
    "\n",
    "    new_ranks = (\n",
    "        verts.join(summed, on=\"node\", how=\"left\")\n",
    "             .select(\n",
    "                 \"node\",\n",
    "                 (F.lit(base + dangling_term) + F.lit(damping) * F.coalesce(F.col(\"sum_contrib\"), F.lit(0.0))).alias(\"rank\")\n",
    "             )\n",
    "             .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    )\n",
    "\n",
    "    ranks.unpersist()\n",
    "    ranks = new_ranks\n",
    "\n",
    "    # Light “plan cut” without checkpoint: materialize every few iters\n",
    "    if (i + 1) % 5 == 0:\n",
    "        _ = ranks.count()\n",
    "\n",
    "ranks.orderBy(F.desc(\"rank\")).limit(10).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0ccc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|id |pagerank          |\n",
      "+---+------------------+\n",
      "|ORD|9.610560458643885 |\n",
      "|ATL|9.26353967070715  |\n",
      "|DEN|9.136974976874244 |\n",
      "|DFW|9.10645536599632  |\n",
      "|MSP|7.2696663196962295|\n",
      "|CLT|6.291784094129735 |\n",
      "|DTW|6.222117232002588 |\n",
      "|IAH|6.103507154206821 |\n",
      "|SLC|5.83094214674235  |\n",
      "|LAX|5.753482547261935 |\n",
      "+---+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# Build vertices (must be column name \"id\")\n",
    "v = (\n",
    "    edge_counts.select(F.col(\"src\").alias(\"id\"))\n",
    "    .union(edge_counts.select(F.col(\"dst\").alias(\"id\")))\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Build edges (must be columns \"src\", \"dst\")\n",
    "e = edge_counts.select(\"src\", \"dst\").distinct()\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Run PageRank (damping=0.85 => resetProbability=0.15)\n",
    "pr = g.pageRank(resetProbability=0.15, maxIter=20)\n",
    "\n",
    "# Top 10 by PageRank centrality\n",
    "pr.vertices.select(\"id\", \"pagerank\") \\\n",
    "  .orderBy(F.desc(\"pagerank\")) \\\n",
    "  .show(10, truncate=False)\n",
    "\n",
    "# Optional: keep for later joins/comparisons\n",
    "ranks_gf = pr.vertices.select(F.col(\"id\").alias(\"node\"), F.col(\"pagerank\").alias(\"rank\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1011ae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undirected edges (unique): 4199\n",
      "Candidate group: top-300 airports by weighted degree => n=300\n",
      "\n",
      "Most connected group (Top-K induced subgraph) summary:\n",
      "  nodes n = 300\n",
      "  edges m = 3979\n",
      "  density = 0.088718   (unweighted undirected density)\n",
      "  total internal weight = 61337065.00\n",
      "\n",
      "Top 20 most connected airports within the group (by internal weighted degree):\n",
      "+---+------------------------+\n",
      "|id |internal_weighted_degree|\n",
      "+---+------------------------+\n",
      "|ATL|7798882.0               |\n",
      "|ORD|5979253.0               |\n",
      "|DFW|5078305.0               |\n",
      "|DEN|4574152.0               |\n",
      "|LAX|4249881.0               |\n",
      "|PHX|3437693.0               |\n",
      "|IAH|3343157.0               |\n",
      "|SFO|3220774.0               |\n",
      "|LAS|2943359.0               |\n",
      "|CLT|2662854.0               |\n",
      "|DTW|2561886.0               |\n",
      "|MSP|2505363.0               |\n",
      "|MCO|2449376.0               |\n",
      "|EWR|2365352.0               |\n",
      "|BOS|2342236.0               |\n",
      "|SLC|2285056.0               |\n",
      "|SEA|2283106.0               |\n",
      "|LGA|2161231.0               |\n",
      "|JFK|2112953.0               |\n",
      "|BWI|2006880.0               |\n",
      "+---+------------------------+\n",
      "\n",
      "\n",
      "Top 20 strongest internal links (by weight) inside the group:\n",
      "+---+---+--------+\n",
      "|u  |v  |w       |\n",
      "+---+---+--------+\n",
      "|LAX|SFO|295359.0|\n",
      "|JFK|LAX|227236.0|\n",
      "|LAS|LAX|224102.0|\n",
      "|HNL|OGG|216280.0|\n",
      "|LGA|ORD|208018.0|\n",
      "|ATL|LGA|187929.0|\n",
      "|ATL|MCO|181334.0|\n",
      "|BOS|DCA|168413.0|\n",
      "|LAX|PHX|166629.0|\n",
      "|LAX|ORD|162694.0|\n",
      "|DEN|PHX|161102.0|\n",
      "|ATL|DFW|159736.0|\n",
      "|BOS|LGA|158415.0|\n",
      "|DEN|LAX|158363.0|\n",
      "|JFK|SFO|157194.0|\n",
      "|LAS|SFO|156456.0|\n",
      "|ATL|FLL|155172.0|\n",
      "|DAL|HOU|154011.0|\n",
      "|HNL|KOA|153485.0|\n",
      "|DEN|SLC|153378.0|\n",
      "+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Group of most connected airports (native Spark, NO GraphFrames)\n",
    "# Method: Top-K hubs + induced subgraph density + top airports inside the group\n",
    "#\n",
    "# Requires:\n",
    "#   edge_counts: DataFrame with columns (src, dst) and optional (weight)\n",
    "#\n",
    "# Outputs:\n",
    "#   - Group summary: n, m, density, total internal weight\n",
    "#   - Top 20 most connected airports within the group (internal weighted degree)\n",
    "#   - Top strongest internal links (by weight)\n",
    "#   - Optional sample of airports in the group\n",
    "# =========================\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Stability knobs for laptops\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")   # avoid plan bloat\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"32\")\n",
    "\n",
    "K = 300                 # try 200 / 300 / 500 (bigger = heavier)\n",
    "TOP_EDGES_TO_SHOW = 20\n",
    "SAMPLE_AIRPORTS_TO_SHOW = 50\n",
    "\n",
    "# Ensure weight exists\n",
    "if \"weight\" not in edge_counts.columns:\n",
    "    edge_counts = edge_counts.withColumn(\"weight\", F.lit(1.0))\n",
    "\n",
    "# 1) Canonical undirected edges: one row per undirected pair (u<v) with aggregated weight\n",
    "undirected = (\n",
    "    edge_counts\n",
    "    .select(\n",
    "        F.when(F.col(\"src\") < F.col(\"dst\"), F.col(\"src\")).otherwise(F.col(\"dst\")).alias(\"u\"),\n",
    "        F.when(F.col(\"src\") < F.col(\"dst\"), F.col(\"dst\")).otherwise(F.col(\"src\")).alias(\"v\"),\n",
    "        F.col(\"weight\").cast(\"double\").alias(\"w\")\n",
    "    )\n",
    "    .filter(F.col(\"u\").isNotNull() & F.col(\"v\").isNotNull() & (F.col(\"u\") != F.col(\"v\")))\n",
    "    .groupBy(\"u\", \"v\")\n",
    "    .agg(F.sum(\"w\").alias(\"w\"))\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "_ = undirected.count()\n",
    "\n",
    "print(\"Undirected edges (unique):\", undirected.count())\n",
    "\n",
    "# 2) Weighted degree: sum of incident weights on u and v\n",
    "deg_u = undirected.groupBy(\"u\").agg(F.sum(\"w\").alias(\"wd\")).withColumnRenamed(\"u\", \"id\")\n",
    "deg_v = undirected.groupBy(\"v\").agg(F.sum(\"w\").alias(\"wd\")).withColumnRenamed(\"v\", \"id\")\n",
    "\n",
    "wdeg = (\n",
    "    deg_u.unionByName(deg_v)\n",
    "         .groupBy(\"id\")\n",
    "         .agg(F.sum(\"wd\").alias(\"weighted_degree\"))\n",
    "         .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "_ = wdeg.count()\n",
    "\n",
    "# 3) Candidate \"most connected group\": top-K airports by weighted degree\n",
    "topK = wdeg.orderBy(F.desc(\"weighted_degree\")).limit(K).select(\"id\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "n0 = topK.count()\n",
    "print(f\"Candidate group: top-{K} airports by weighted degree => n={n0}\")\n",
    "\n",
    "# 4) Induced edges among topK (still undirected u<v)\n",
    "topK_u = topK.withColumnRenamed(\"id\", \"u\")\n",
    "topK_v = topK.withColumnRenamed(\"id\", \"v\")\n",
    "\n",
    "E = (\n",
    "    undirected.join(topK_u, on=\"u\", how=\"inner\")\n",
    "              .join(topK_v, on=\"v\", how=\"inner\")\n",
    "              .select(\"u\", \"v\", \"w\")\n",
    "              .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "m = E.count()\n",
    "\n",
    "# Vertices actually present in induced subgraph (some topK may be isolated within topK)\n",
    "V = (\n",
    "    E.select(F.col(\"u\").alias(\"id\"))\n",
    "     .union(E.select(F.col(\"v\").alias(\"id\")))\n",
    "     .distinct()\n",
    "     .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "n = V.count()\n",
    "\n",
    "density = (2.0 * m) / (n * (n - 1)) if n > 1 else 0.0\n",
    "total_w = E.agg(F.sum(\"w\").alias(\"tw\")).first()[\"tw\"]\n",
    "total_w = float(total_w) if total_w is not None else 0.0\n",
    "\n",
    "print(\"\\nMost connected group (Top-K induced subgraph) summary:\")\n",
    "print(f\"  nodes n = {n}\")\n",
    "print(f\"  edges m = {m}\")\n",
    "print(f\"  density = {density:.6f}   (unweighted undirected density)\")\n",
    "print(f\"  total internal weight = {total_w:.2f}\")\n",
    "\n",
    "# 5) \"Most connected airports\" INSIDE the group (internal weighted degree)\n",
    "# Internal weighted degree = sum of edge weights incident to the airport, restricted to edges inside E.\n",
    "deg_u_in = E.groupBy(\"u\").agg(F.sum(\"w\").alias(\"internal_wdeg\")).withColumnRenamed(\"u\", \"id\")\n",
    "deg_v_in = E.groupBy(\"v\").agg(F.sum(\"w\").alias(\"internal_wdeg\")).withColumnRenamed(\"v\", \"id\")\n",
    "\n",
    "internal_wdeg = (\n",
    "    deg_u_in.unionByName(deg_v_in)\n",
    "            .groupBy(\"id\")\n",
    "            .agg(F.sum(\"internal_wdeg\").alias(\"internal_weighted_degree\"))\n",
    "            .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 20 most connected airports within the group (by internal weighted degree):\")\n",
    "internal_wdeg.orderBy(F.desc(\"internal_weighted_degree\")).limit(20).show(truncate=False)\n",
    "\n",
    "# 6) Strongest internal links (justify why this is a tightly connected group)\n",
    "print(f\"\\nTop {TOP_EDGES_TO_SHOW} strongest internal links (by weight) inside the group:\")\n",
    "E.orderBy(F.desc(\"w\")).limit(TOP_EDGES_TO_SHOW).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735933db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Build GraphFrame (for comparison only)\n",
    "# -----------------------------\n",
    "v_gf = edge_counts.select(F.col(\"src\").alias(\"id\")).union(edge_counts.select(F.col(\"dst\").alias(\"id\"))).distinct()\n",
    "e_gf = edge_counts.select(\"src\", \"dst\").distinct()\n",
    "g = GraphFrame(v_gf, e_gf)\n",
    "\n",
    "# GraphFrames degree (undirected total degree)\n",
    "gf_deg = g.degrees.select(\"id\", F.col(\"degree\").alias(\"degree_gf\"))\n",
    "\n",
    "# GraphFrames PageRank (comparison only)\n",
    "gf_pr = g.pageRank(resetProbability=0.15, maxIter=20).vertices \\\n",
    "         .select(\"id\", F.col(\"pagerank\").alias(\"pagerank_gf\"))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Your native results\n",
    "# -----------------------------\n",
    "native_deg = degrees.select(\"id\", \"in_degree\", \"out_degree\", \"total_degree\")\n",
    "\n",
    "native_evec = evec.select(F.col(\"node\").alias(\"id\"), F.col(\"eigen_c\").alias(\"eigen_c_native\"))\n",
    "\n",
    "native_pr = ranks.select(F.col(\"node\").alias(\"id\"), F.col(\"rank\").alias(\"pagerank_native\"))\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Join and compare\n",
    "# -----------------------------\n",
    "comparison = (native_deg\n",
    "              .join(native_evec, \"id\", \"left\")\n",
    "              .join(native_pr, \"id\", \"left\")\n",
    "              .join(gf_deg, \"id\", \"left\")\n",
    "              .join(gf_pr, \"id\", \"left\")\n",
    "              .na.fill(0))\n",
    "\n",
    "print(\"\\nTop 20 by native PageRank vs GraphFrames PageRank:\")\n",
    "comparison.orderBy(F.desc(\"pagerank_native\")).select(\n",
    "    \"id\", \"pagerank_native\", \"pagerank_gf\", \"total_degree\", \"degree_gf\", \"eigen_c_native\"\n",
    ").limit(20).show(truncate=False)\n",
    "\n",
    "print(\"\\nTop 20 by GraphFrames PageRank:\")\n",
    "comparison.orderBy(F.desc(\"pagerank_gf\")).select(\n",
    "    \"id\", \"pagerank_gf\", \"pagerank_native\", \"total_degree\", \"degree_gf\", \"eigen_c_native\"\n",
    ").limit(20).show(truncate=False)\n",
    "\n",
    "print(\"\\nTop 20 by native total_degree vs GraphFrames degree:\")\n",
    "comparison.orderBy(F.desc(\"total_degree\")).select(\n",
    "    \"id\", \"total_degree\", \"degree_gf\", \"pagerank_native\", \"pagerank_gf\", \"eigen_c_native\"\n",
    ").limit(20).show(truncate=False)\n",
    "\n",
    "print(\"\\nTop 20 by eigenvector centrality (native):\")\n",
    "comparison.orderBy(F.desc(\"eigen_c_native\")).select(\n",
    "    \"id\", \"eigen_c_native\", \"total_degree\", \"degree_gf\", \"pagerank_native\", \"pagerank_gf\"\n",
    ").limit(20).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d12ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: draw a heatmap of flights between top airports\n",
    "# This will collect a small dense matrix to the driver — do it only for top-K airports.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "top_k = 30\n",
    "top_airports = degrees.orderBy(F.desc(\"total_degree\")).limit(top_k).select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "# filter edges to top airports and pivot to matrix\n",
    "sub = edge_counts.filter((F.col(\"src\").isin(top_airports)) & (F.col(\"dst\").isin(top_airports)))\n",
    "pdf = sub.toPandas()\n",
    "mat = pd.pivot_table(pdf, values='weight', index='src', columns='dst', fill_value=0)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(mat.values, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(mat.columns)), mat.columns, rotation=90)\n",
    "plt.yticks(range(len(mat.index)), mat.index)\n",
    "plt.title(\"Heatmap of flights between top {} airports\".format(top_k))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a0217-3d60-4ae3-8d4c-0dd6409e38a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
